{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from numpy import *\n",
    "import os\n",
    "\n",
    "\n",
    "def get_tf_list(tf_path):\n",
    "    # return tf_list\n",
    "    f_tf = open(tf_path)\n",
    "    tf_reader = list(csv.reader(f_tf))\n",
    "    tf_list = []\n",
    "    for single in tf_reader[1:]:\n",
    "        tf_list.append(single[0])\n",
    "    print('Load ' + str(len(tf_list)) + ' TFs successfully!')\n",
    "    return tf_list\n",
    "\n",
    "def get_origin_expression_data(gene_expression_path):\n",
    "    # return 1.tf-targets dict and pair-score dict\n",
    "    #        2.number of timepoints\n",
    "    f_expression = open(gene_expression_path, encoding=\"utf-8\")\n",
    "    expression_reader = list(csv.reader(f_expression))\n",
    "    cells = expression_reader[0][1:]\n",
    "    num_cells = len(cells)\n",
    "\n",
    "    expression_record = {}\n",
    "    num_genes = 0\n",
    "    for single_expression_reader in expression_reader[1:]:\n",
    "        if single_expression_reader[0] in expression_record:\n",
    "            print('Gene name ' + single_expression_reader[0] + ' repeat!')\n",
    "        expression_record[single_expression_reader[0]] = list(map(float, single_expression_reader[1:]))\n",
    "        num_genes += 1\n",
    "    print(str(num_genes) + ' genes and ' + str(num_cells) + ' cells are included in origin expression data.')\n",
    "    return expression_record, cells\n",
    "\n",
    "def get_normalized_expression_data(gene_expression_path):\n",
    "    # return 1.tf-targets dict and pair-score dict\n",
    "    #        2.number of timepoints\n",
    "    expression_record, cells = get_origin_expression_data(gene_expression_path)\n",
    "    expression_matrix = np.zeros((len(expression_record), len(cells)))\n",
    "    index_row = 0\n",
    "    for gene in expression_record:\n",
    "        expression_record[gene] = np.log10(np.array(expression_record[gene]) + 10 ** -2)\n",
    "        expression_matrix[index_row] = expression_record[gene]\n",
    "        index_row += 1\n",
    "\n",
    "    # Heat map\n",
    "    # plt.figure(figsize=(15,15))\n",
    "    # sns.heatmap(expression_matrix[0:100,0:100])\n",
    "    # plt.show()\n",
    "\n",
    "    return expression_record, cells\n",
    "\n",
    "def get_gene_ranking(gene_order_path, low_express_gene_list, gene_num, output_path,\n",
    "                     flag):  # flag=True:write to output_path\n",
    "    # 1.delete genes p-value>=0.01\n",
    "    # 2.delete genes with low expression\n",
    "    # 3.rank genes in descending order of variance\n",
    "    # 4.return gene names list of top genes and variance_record of p-value<0.01\n",
    "    f_order = open(gene_order_path)\n",
    "    order_reader = list(csv.reader(f_order))\n",
    "    if flag:\n",
    "        f_rank = open(output_path, 'w', newline='\\n')\n",
    "        f_rank_writer = csv.writer(f_rank)\n",
    "    variance_record = {}\n",
    "    variance_list = []\n",
    "    significant_gene_list = []\n",
    "    for single_order_reader in order_reader[1:]:\n",
    "        # column 0:gene name\n",
    "        # column 1:p value\n",
    "        # column 2:variance\n",
    "        if float(single_order_reader[1]) >= 0.01:\n",
    "            break\n",
    "        if single_order_reader[0] in low_express_gene_list:\n",
    "            continue\n",
    "        variance = float(single_order_reader[2])\n",
    "        if variance not in variance_record:  # 1 variance corresponding to 1 gene\n",
    "            variance_record[variance] = single_order_reader[0]\n",
    "        else:  # 1 variance corresponding to n genes\n",
    "            print(str(variance_record[variance]) + ' and ' + single_order_reader[0] + ' variance repeat!')\n",
    "            variance_record[variance] = [variance_record[variance]]\n",
    "            variance_record[variance].append(single_order_reader[0])\n",
    "        variance_list.append(variance)\n",
    "        tstr = single_order_reader[0]\n",
    "        single_order_reader[0] = tstr.upper()\n",
    "        significant_gene_list.append(single_order_reader[0])\n",
    "    print('After delete genes with p-value>=0.01 or low expression, ' + str(len(variance_list)) + ' genes left.')\n",
    "    variance_list.sort(reverse=True)\n",
    "    gene_rank = []\n",
    "    for single_variance_list in variance_list[0:gene_num]:\n",
    "        if type(variance_record[single_variance_list]) is str:  # 1 variance corresponding to 1 gene\n",
    "            gene_rank.append(variance_record[single_variance_list])\n",
    "        else:  # 1 variance corresponding to n genes\n",
    "            gene_rank.append(variance_record[single_variance_list][0])\n",
    "            del variance_record[single_variance_list][0]\n",
    "            if len(variance_record[single_variance_list]) == 1:\n",
    "                variance_record[single_variance_list] = variance_record[single_variance_list][0]\n",
    "        if flag:\n",
    "            f_rank_writer.writerow([variance_record[single_variance_list]])\n",
    "    f_order.close()\n",
    "    if flag:\n",
    "        f_rank.close()\n",
    "    return gene_rank, significant_gene_list\n",
    "\n",
    "def get_filtered_gold(gold_network_path, rank_list, output_path, flag):\n",
    "    # 1.Load origin gold file\n",
    "    # 2.Delete genes not in rank_list\n",
    "    # 3.return tf-targets dict and pair-score dict\n",
    "    # Note: If no score in gold network, score=999\n",
    "    f_gold = open(gold_network_path, encoding='UTF-8-sig')\n",
    "    gold_reader = list(csv.reader(f_gold))\n",
    "    for i in range(0, len(gold_reader) - 1):\n",
    "        temp = gold_reader[i]\n",
    "        s1 = str(temp[0])\n",
    "        s2 = str(temp[1])\n",
    "\n",
    "        temp[0] = s1.upper()\n",
    "        temp[1] = s2.upper()\n",
    "\n",
    "        gold_reader[i] = temp\n",
    "    # print(\"gold_reader\",gold_reader)\n",
    "    # print(\"rank_list\",rank_list)\n",
    "    # print(\"gold_reader\",gold_reader)\n",
    "    print(\"gold_reader[0]\", gold_reader[0])\n",
    "    has_score = True\n",
    "    if len(gold_reader[0]) < 3:\n",
    "        has_score = False\n",
    "    gold_pair_record = {}\n",
    "    gold_score_record = {}\n",
    "    unique_gene_list = []\n",
    "    for single_gold_reader in gold_reader[1:]:\n",
    "        # column 0: TF\n",
    "        # column 1: target gene\n",
    "        # column 2: regulate score\n",
    "        if (single_gold_reader[0] not in rank_list) or (single_gold_reader[1] not in rank_list):\n",
    "            continue\n",
    "        gene_pair = [single_gold_reader[0], single_gold_reader[1]]\n",
    "        str_gene_pair = single_gold_reader[0] + ',' + single_gold_reader[1]\n",
    "\n",
    "        if single_gold_reader[0] not in unique_gene_list: unique_gene_list.append(single_gold_reader[0])\n",
    "        if single_gold_reader[1] not in unique_gene_list: unique_gene_list.append(single_gold_reader[1])\n",
    "        if str_gene_pair in gold_score_record:\n",
    "            print('Gold pair repeat!')\n",
    "        if has_score:\n",
    "            print(\"single_gold_reader[2]\", single_gold_reader[2])\n",
    "            gold_score_record[str_gene_pair] = float(single_gold_reader[2])\n",
    "        else:\n",
    "            gold_score_record[str_gene_pair] = 999\n",
    "        if gene_pair[0] not in gold_pair_record:\n",
    "            gold_pair_record[gene_pair[0]] = [gene_pair[1]]\n",
    "        else:\n",
    "            gold_pair_record[gene_pair[0]].append(gene_pair[1])\n",
    "    print(\"gold_pair_record\", gold_pair_record)\n",
    "    # Some statistics of gold_network\n",
    "    print(str(len(gold_pair_record)) + ' TFs and ' + str(\n",
    "        len(gold_score_record)) + ' edges in gold_network consisted of genes in rank_list.')\n",
    "    print(str(len(unique_gene_list)) + ' genes are common in rank_list and gold_network.')\n",
    "\n",
    "    rank_density = len(gold_score_record) / (len(gold_pair_record) * (len(rank_list)))\n",
    "    gold_density = len(gold_score_record) / (len(gold_pair_record) * (len(unique_gene_list)))\n",
    "\n",
    "    print('Rank genes density = edges/(TFs*(len(rank_gene)-1))=' + str(rank_density))\n",
    "    print('Gold genes density = edges/(TFs*len(unique_gene_list))=' + str(gold_density))\n",
    "\n",
    "    # write to file\n",
    "    print(\"unique_gene_list\", unique_gene_list)\n",
    "    if flag:\n",
    "        f_unique = open(output_path, 'w', encoding=\"utf-8\", newline='\\n')\n",
    "        f_unique_writer = csv.writer(f_unique)\n",
    "        out_unique = np.array(unique_gene_list).reshape(len(unique_gene_list), 1)\n",
    "        f_unique_writer.writerows(out_unique)\n",
    "        f_unique.close()\n",
    "    return gold_pair_record, gold_score_record, unique_gene_list\n",
    "\n",
    "def generate_filtered_gold(gold_pair_record, gold_score_record, output_path):\n",
    "    # write filtered_gold to output_path\n",
    "    # print(\"cnm\")\n",
    "    f_filtered = open(output_path, 'w', encoding=\"utf-8\", newline='\\n')\n",
    "    f_filtered_writer = csv.writer(f_filtered)\n",
    "    f_filtered_writer.writerow(['TF', 'Target', 'Score'])\n",
    "    # print(\"cnm\")\n",
    "    for tf in gold_pair_record:\n",
    "        once_output = []\n",
    "        for target in gold_pair_record[tf]:\n",
    "            single_output = [tf, target, gold_score_record[tf + ',' + target]]\n",
    "            once_output.append(single_output)\n",
    "        f_filtered_writer.writerows(once_output)\n",
    "    f_filtered.close()\n",
    "\n",
    "def get_gene_pair_list(unique_gene_list, gold_pair_record, gold_score_record, output_file):\n",
    "    # positive is relationship that tf regulate target\n",
    "    # negtive is reationship that same tf doesn's regulate target.\n",
    "    # When same tf doesn't have enough negtive, borrow negtive from other TFs.\n",
    "    # When negtive is not enough,stop and prove positive:negtive = 1:1\n",
    "\n",
    "    # generate all negtive gene pairs of TFs\n",
    "    all_tf_negtive_record = {}\n",
    "    for tf in gold_pair_record:\n",
    "        # print(\"tf\",tf)\n",
    "        all_tf_negtive_record[tf] = []\n",
    "        for target in unique_gene_list:\n",
    "            if target in gold_pair_record[tf]:\n",
    "                continue\n",
    "            all_tf_negtive_record[tf].append(target)\n",
    "\n",
    "    # generate negtive record without borrow\n",
    "    rank_negtive_record = {}\n",
    "    for tf in gold_pair_record:\n",
    "        num_positive = len(gold_pair_record[tf])\n",
    "        if num_positive > len(all_tf_negtive_record[tf]):\n",
    "            rank_negtive_record[tf] = all_tf_negtive_record[tf]\n",
    "            all_tf_negtive_record[tf] = []\n",
    "        else:\n",
    "            # maybe random.sample(all_tf_negtive_record[tf],num_positive) to promote performance\n",
    "            rank_negtive_record[tf] = all_tf_negtive_record[tf][:num_positive]\n",
    "            all_tf_negtive_record[tf] = all_tf_negtive_record[tf][num_positive:]\n",
    "\n",
    "    # output positive and negtive pairs\n",
    "    f_gpl = open(output_file, 'w', newline='\\n')\n",
    "    f_gpl_writer = csv.writer(f_gpl)\n",
    "    f_gpl_writer.writerow(['TF', 'Target', 'Label', 'Score'])\n",
    "    stop_flag = False\n",
    "    for tf in gold_pair_record:\n",
    "        once_output = []\n",
    "        for target in gold_pair_record[tf]:\n",
    "            # output positive\n",
    "            single_output = [tf, target, '1', gold_score_record[tf + ',' + target]]\n",
    "            once_output.append(single_output)\n",
    "            # output negtive\n",
    "            if len(rank_negtive_record[tf]) == 0:\n",
    "                # borrow negtive for other TFs\n",
    "                find_negtive = False\n",
    "                for borrow_tf in all_tf_negtive_record:\n",
    "                    if len(all_tf_negtive_record[borrow_tf]) > 0:\n",
    "                        find_negtive = True\n",
    "                        single_output = [borrow_tf, all_tf_negtive_record[borrow_tf][0], 0, 0]\n",
    "                        del all_tf_negtive_record[borrow_tf][0]\n",
    "                        break\n",
    "                # if not enough negtive of others,stop and prove positive:negtive = 1:1\n",
    "                if not find_negtive:\n",
    "                    stop_flag = True\n",
    "                    break\n",
    "            else:\n",
    "                # negtive without borrow\n",
    "                single_output = [tf, rank_negtive_record[tf][0], 0, 0]\n",
    "                del rank_negtive_record[tf][0]\n",
    "            once_output.append(single_output)\n",
    "        if stop_flag:\n",
    "            f_gpl_writer.writerows(once_output[:-1])\n",
    "            print('Negtive not enough!')\n",
    "            break\n",
    "        f_gpl_writer.writerows(once_output)  # output positive and negtive of 1 TF at a time\n",
    "    f_gpl.close()\n",
    "\n",
    "def get_low_express_gene(origin_expression_record, num_cells):\n",
    "    # get gene_list who were expressed in fewer than 10% of the cells\n",
    "    gene_list = []\n",
    "    threshold = num_cells // 10\n",
    "    for gene in origin_expression_record:\n",
    "        num = 0\n",
    "        for expression in origin_expression_record[gene]:\n",
    "            if expression != 0:\n",
    "                num += 1\n",
    "                if num > threshold:\n",
    "                    break\n",
    "        if num <= threshold:\n",
    "            gene_list.append(gene)\n",
    "    return gene_list\n",
    "\n",
    "\n",
    "\n",
    "def loadData(dataset,gene_pair_list_path,gene_expression_path,resultPath):\n",
    "\n",
    "    origin_expression_record, cells = get_normalized_expression_data(gene_expression_path)\n",
    "    print(\"len(origin_expression_record)\", len(origin_expression_record))\n",
    "\n",
    "    # Load gold_pair_record\n",
    "    all_gene_list = []\n",
    "    gold_pair_record = {}\n",
    "    f_genePairList = open(gene_pair_list_path, encoding='UTF-8')  ### read the gene pair and label file\n",
    "\n",
    "    for single_pair in list(csv.reader(f_genePairList))[1:]:\n",
    "        print(\"single_pair\",single_pair)\n",
    "        if single_pair[2] == '1':\n",
    "            if single_pair[0] not in gold_pair_record:\n",
    "                gold_pair_record[single_pair[0]] = [single_pair[1]]\n",
    "            else:\n",
    "                gold_pair_record[single_pair[0]].append(single_pair[1])\n",
    "            # count all genes in gold edges\n",
    "            if single_pair[0] not in all_gene_list:\n",
    "                all_gene_list.append(single_pair[0])\n",
    "            if single_pair[1] not in all_gene_list:\n",
    "                all_gene_list.append(single_pair[1])\n",
    "    f_genePairList.close()\n",
    "    # print dataset statistics\n",
    "    print('All genes:' + str(len(all_gene_list)))\n",
    "    print('TFs:' + str(len(gold_pair_record.keys())))\n",
    "    print(\"len(single_pair)\", len(single_pair))\n",
    "    # Generate Pearson matrix\n",
    "    label_list = []\n",
    "    pair_list = []\n",
    "    total_matrix = []\n",
    "    num_tf = -1\n",
    "    num_label1 = 0\n",
    "    num_label0 = 0\n",
    "\n",
    "    # control cell numbers by means of timepoints\n",
    "    timepoints = len(cells)\n",
    "    # timepoints=800\n",
    "    x = []\n",
    "    for i in gold_pair_record:\n",
    "        num_tf += 1\n",
    "        for j in range(len(all_gene_list)):\n",
    "            # for j in range(2):\n",
    "            print('Generating matrix of gene pair ' + str(num_tf) + ' ' + str(j))\n",
    "            tf_name = i\n",
    "            target_name = all_gene_list[j]\n",
    "\n",
    "            flag = False\n",
    "            if (origin_expression_record.__contains__(tf_name) & origin_expression_record.__contains__(target_name)):\n",
    "                flag = True\n",
    "\n",
    "            if (flag):\n",
    "                if tf_name in gold_pair_record and target_name in gold_pair_record[tf_name]:\n",
    "                    label = 1\n",
    "                    num_label1 += 1\n",
    "                else:\n",
    "                    label = 0\n",
    "                    num_label0 += 1\n",
    "                label_list.append(label)\n",
    "                pair_list.append(tf_name + ',' + target_name)\n",
    "\n",
    "                tf_data = origin_expression_record[tf_name]\n",
    "                target_data = origin_expression_record[target_name]\n",
    "            else:\n",
    "                miss = miss + 1\n",
    "                continue\n",
    "\n",
    "            single_tf_list = []\n",
    "            gap = 100\n",
    "            for k in range(0, len(tf_data), gap):\n",
    "                feature = []\n",
    "                a = tf_data[k:k + gap]\n",
    "                b = target_data[k:k + gap]\n",
    "                feature.extend(a)\n",
    "                feature.extend(b)\n",
    "                # single_tf_list.append(feature)\n",
    "                feature = np.asarray(feature)\n",
    "                # print(\"feature.shape\", feature.shape)\n",
    "                if (len(feature) == 2 * gap):\n",
    "                    # print(\"feature.shape xixihaha\", feature.shape)\n",
    "                    single_tf_list.append(feature)\n",
    "\n",
    "            single_tf_list = np.asarray(single_tf_list)\n",
    "\n",
    "            total_matrix.append(single_tf_list)\n",
    "\n",
    "    total_matrix = np.asarray(total_matrix)\n",
    "    label_list = np.array(label_list)\n",
    "    # print(\"label_list.shape\", label_list.shape)\n",
    "    pair_list = np.array(pair_list)\n",
    "\n",
    "    np.save(resultPath + 'matrix.npy', total_matrix)\n",
    "    np.save(resultPath + 'label.npy', label_list)\n",
    "    np.save(resultPath + 'gene_pair.npy', pair_list)\n",
    "\n",
    "    print('PCC matrix generation finish.')\n",
    "    print('Positive edges:' + str(num_label1))\n",
    "    print('Negative edges:' + str(num_label0))\n",
    "    print('Density=' + str(num_label1 / (num_label1 + num_label0)))\n",
    "##generating the data can be inputted by the STGRNS\n",
    "\n",
    "dataset = \"06hHep_ExpressionDataOrdered.csv\"\n",
    "\n",
    "known_network = \"known_network.csv\"\n",
    "gene_pair_list_path = \"Data/exampleData/\" + known_network\n",
    "gene_expression_path = \"Data/exampleData/\" + dataset\n",
    "resultPath = \"Data/exampleData/input/\"\n",
    "loadData(dataset,gene_pair_list_path,gene_expression_path,resultPath)\n",
    "\n",
    "\n",
    "unknown_network = \"unknown_network.csv\"\n",
    "gene_pair_list_path = \"Data/exampleData/\" + unknown_network\n",
    "gene_expression_path = \"Data/exampleData/\" + dataset\n",
    "resultPath = \"Data/exampleData/input/\"\n",
    "loadData(dataset,gene_pair_list_path,gene_expression_path,resultPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc7e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "from torch.utils.data import (DataLoader)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "def numpy2loader(X, y, batch_size):\n",
    "    X_set = torch.from_numpy(X)\n",
    "    X_loader = DataLoader(X_set, batch_size=batch_size)\n",
    "    y_set = torch.from_numpy(y)\n",
    "    y_loader = DataLoader(y_set, batch_size=batch_size)\n",
    "\n",
    "    return X_loader, y_loader\n",
    "\n",
    "def loaderToList(data_loader):\n",
    "    length = len(data_loader)\n",
    "    data = []\n",
    "    for i in data_loader:\n",
    "        data.append(i)\n",
    "    return data\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class STGRNS(nn.Module):\n",
    "    def __init__(self, input_dim, nhead=2, d_model=80, num_classes=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.prenet = nn.Linear(input_dim, d_model)\n",
    "        self.positionalEncoding = PositionalEncoding(d_model=d_model, dropout=dropout)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, dim_feedforward=256, nhead=2, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.pred_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, window_size):\n",
    "        out = window_size.permute(1, 0, 2)\n",
    "        out = self.positionalEncoding(out)\n",
    "        out = self.encoder_layer(out)\n",
    "        out = out.transpose(0, 1)\n",
    "        stats = out.mean(dim=1)\n",
    "        out = self.pred_layer(stats)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def STGRNSForGRNSRconstruction(batch_sizes, epochs,known_data_path,unknown_data_path,num_threads):\n",
    "    data_path = known_data_path\n",
    "    d_models = epochs\n",
    "    torch.set_num_threads(num_threads) #set num_threads\n",
    "    batch_size = batch_sizes\n",
    "    log_dir = \"log/\"\n",
    "    if (not os.path.isdir(log_dir)):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    x_train = np.load(data_path + 'matrix.npy')\n",
    "    y_train = np.load(data_path + 'label.npy')\n",
    "\n",
    "    X_trainloader, y_trainloader = numpy2loader(x_train, y_train, batch_size)\n",
    "\n",
    "    X_trainList = loaderToList(X_trainloader)\n",
    "    y_trainList = loaderToList(y_trainloader)\n",
    "\n",
    "    model = STGRNS(input_dim=200, nhead=2, d_model=d_models, num_classes=2)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
    "\n",
    "    n_epochs = epochs\n",
    "    acc_record = {'train': [], 'dev': []}\n",
    "    loss_record = {'train': [], 'dev': []}\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = []\n",
    "        for j in range(0, len(X_trainList)):\n",
    "            data = X_trainList[j]\n",
    "            labels = y_trainList[j]\n",
    "            logits = model(data)\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "            optimizer.step()\n",
    "            acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "            train_loss.append(loss.item())\n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        loss_record['train'].append(train_loss)\n",
    "\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}\")\n",
    "\n",
    "    ###predict-----------------------------------------------\n",
    "    y_predict = []\n",
    "    data_path = unknown_data_path\n",
    "    x_test = np.load(data_path + 'matrix.npy')\n",
    "    y_test = np.load(data_path + 'label.npy')\n",
    "\n",
    "    X_testloader, y_testloader = numpy2loader(x_test, y_test, batch_size)\n",
    "\n",
    "    X_testList = loaderToList(X_testloader)\n",
    "    y_testList = loaderToList(y_testloader)\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for k in range(0, len(X_testList)):\n",
    "        data = X_testList[k]\n",
    "        with torch.no_grad():\n",
    "            logits = model(data)\n",
    "        predt = F.softmax(logits)\n",
    "        temps = predt.cpu().numpy().tolist()\n",
    "        for i in temps:\n",
    "            t = i[1]\n",
    "            y_predict.append(t)\n",
    "\n",
    "    # print(\"y_predict\", y_predict)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predict, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    precision, recall, thresholds_PR = metrics.precision_recall_curve(y_test, y_predict)\n",
    "    AUPR = metrics.auc(recall, precision)\n",
    "    y_predict2 = []\n",
    "    for pre in y_predict:\n",
    "        if(pre >0.001):\n",
    "            y_predict2.append(1)\n",
    "        else:\n",
    "            y_predict2.append(0)\n",
    "    acc = metrics.accuracy_score(y_test, y_predict2)\n",
    "    bacc = metrics.balanced_accuracy_score(y_test, y_predict2)\n",
    "    f1 = metrics.f1_score(y_test, y_predict2)\n",
    "\n",
    "    ##storing the predicted data\n",
    "    np.save(log_dir + 'y_test.npy', y_test)\n",
    "    np.save(log_dir + 'y_predict.npy', y_predict)\n",
    "    \n",
    "     ##storing the predicted network\n",
    "    np.save(log_dir + 'y_predict2.npy', y_predict2)\n",
    "\n",
    "\n",
    "##the data path of known data\n",
    "data_path = 'Data/exampleData/input/known/'\n",
    "\n",
    "##the data path of unknown data\n",
    "unknown_data_path = 'Data/exampleData/input/unknow/'\n",
    "num_threads = 1\n",
    "##training model and then predicting unknown network\n",
    "batch_sizes = 32\n",
    "epochs = 200\n",
    "STGRNSForGRNSRconstruction(batch_sizes,epochs,data_path,unknown_data_path,num_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
